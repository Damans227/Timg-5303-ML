{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prerequisites â€“ Download nltk stopwords and spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgetanev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/georgetanev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords'); nltk.download('wordnet')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgetanev/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en import English\n",
    "\n",
    "parser = English()\n",
    "ss = SnowballStemmer(\"english\")\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgetanev/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "def getDocTopicWeight(lda_model_in, new_doc):\n",
    "    doc = prepare_text_for_lda(new_doc)\n",
    "    new_doc_bow = id2word.doc2bow(doc)\n",
    "    return lda_model_in.get_document_topics(new_doc_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "sw = pd.read_csv('VP_assertions_stop_words_5303_W2021.txt')\n",
    "\n",
    "# Convert to list\n",
    "sw_data = sw.Archive.tolist()\n",
    "# stop_words.extend(sw_data)\n",
    "stop_words = sw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Assrtions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('230 VP assertions corpus - Group 6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G6A002</td>\n",
       "      <td>Access resources required to scale at relative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A003</td>\n",
       "      <td>Adapt offers to each market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A005</td>\n",
       "      <td>Align interests of investors, the company top ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A006</td>\n",
       "      <td>Allow resource owners to make money using your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A007</td>\n",
       "      <td>Apply big data analytics to produce insightful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name                                            content\n",
       "0  G6A002  Access resources required to scale at relative...\n",
       "1    A003                        Adapt offers to each market\n",
       "2    A005  Align interests of investors, the company top ...\n",
       "3    A006  Allow resource owners to make money using your...\n",
       "4    A007  Apply big data analytics to produce insightful..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Access resources required to scale at relatively low cost or for free by '\n",
      " 'creating benefits for the resource owners that they cannot create alone']\n"
     ]
    }
   ],
   "source": [
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words and Clean-up text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['access', 'resources', 'required', 'to', 'scale', 'at', 'relatively', 'low', 'cost', 'or', 'for', 'free', 'by', 'creating', 'benefits', 'for', 'the', 'resource', 'owners', 'that', 'they', 'cannot', 'create', 'alone']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize each sentence into a list of words, removing punctuations and unnecessary characters \n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'resources', 'required', 'to', 'scale', 'at', 'relatively', 'low', 'cost', 'or', 'for', 'free', 'by', 'creating', 'benefits', 'for', 'the', 'resource_owners', 'that', 'they', 'cannot', 'create', 'alone']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords, Make Bigrams & Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def stem_words(texts):\n",
    "    return [[ss.stem(word) for word in simple_preprocess(str(doc))] for doc in texts]\n",
    "\n",
    "def lem_words(texts):\n",
    "    return [[lem.lemmatize(word) for word in simple_preprocess(str(doc))] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data)\n",
    "\n",
    "#data_stemmed = stem_words(data_words_nostops)\n",
    "data_lemmatized = lem_words(data_words_nostops)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resource', 'low', 'benefit', 'resource', 'owner']]\n"
     ]
    }
   ],
   "source": [
    "pprint(data_words_bigrams[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data_words_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 2)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benefit'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('benefit', 1), ('low', 1), ('owner', 1), ('resource', 2)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.601*\"channel\" + 0.001*\"cybersecurity\" + 0.001*\"offer\" + 0.001*\"resource\" '\n",
      "  '+ 0.001*\"stream\" + 0.001*\"tend\" + 0.001*\"advocacy\" + 0.001*\"aligned\" + '\n",
      "  '0.001*\"initiative\" + 0.001*\"preferentially\"'),\n",
      " (1,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (2,\n",
      "  '0.287*\"master\" + 0.002*\"resource\" + 0.002*\"owner\" + 0.002*\"aligned\" + '\n",
      "  '0.002*\"loyal\" + 0.002*\"stream\" + 0.002*\"wish\" + 0.002*\"tend\" + '\n",
      "  '0.002*\"initiative\" + 0.002*\"customizing\"'),\n",
      " (3,\n",
      "  '0.902*\"supplier\" + 0.000*\"customer\" + 0.000*\"proposition\" + '\n",
      "  '0.000*\"advocacy\" + 0.000*\"wish\" + 0.000*\"treatment\" + 0.000*\"aligned\" + '\n",
      "  '0.000*\"tend\" + 0.000*\"customizing\" + 0.000*\"stream\"'),\n",
      " (4,\n",
      "  '0.302*\"owner\" + 0.287*\"resource\" + 0.176*\"investor\" + 0.026*\"proposition\" + '\n",
      "  '0.002*\"customer\" + 0.001*\"return\" + 0.001*\"benefit\" + 0.001*\"loyal\" + '\n",
      "  '0.001*\"stream\" + 0.001*\"tend\"'),\n",
      " (5,\n",
      "  '0.079*\"low\" + 0.003*\"owner\" + 0.003*\"resource\" + 0.003*\"benefit\" + '\n",
      "  '0.003*\"product\" + 0.003*\"service\" + 0.003*\"tend\" + 0.003*\"loyal\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"wish\"'),\n",
      " (6,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (7,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (8,\n",
      "  '0.074*\"term\" + 0.074*\"bound\" + 0.003*\"investment\" + 0.003*\"benefit\" + '\n",
      "  '0.003*\"stakeholder\" + 0.003*\"initiative\" + 0.003*\"tend\" + 0.003*\"loyal\" + '\n",
      "  '0.003*\"treatment\" + 0.003*\"aligned\"'),\n",
      " (9,\n",
      "  '0.365*\"partner\" + 0.245*\"company\" + 0.127*\"customer\" + 0.111*\"standard\" + '\n",
      "  '0.031*\"community\" + 0.010*\"cyberattacks\" + 0.000*\"benefit\" + '\n",
      "  '0.000*\"investment\" + 0.000*\"tend\" + 0.000*\"aligned\"'),\n",
      " (10,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (11,\n",
      "  '0.835*\"customer\" + 0.049*\"data\" + 0.036*\"habit\" + 0.011*\"trend\" + '\n",
      "  '0.005*\"pattern\" + 0.005*\"solution\" + 0.005*\"payment\" + 0.000*\"offer\" + '\n",
      "  '0.000*\"user\" + 0.000*\"service\"'),\n",
      " (12,\n",
      "  '0.307*\"opportunity\" + 0.227*\"investment\" + 0.207*\"benefit\" + '\n",
      "  '0.095*\"investor\" + 0.060*\"existing\" + 0.000*\"customer\" + 0.000*\"stream\" + '\n",
      "  '0.000*\"advocacy\" + 0.000*\"aligned\" + 0.000*\"tend\"'),\n",
      " (13,\n",
      "  '0.245*\"cybersecurity\" + 0.025*\"compared\" + 0.002*\"offer\" + '\n",
      "  '0.002*\"competitor\" + 0.002*\"proposition\" + 0.002*\"owner\" + 0.002*\"resource\" '\n",
      "  '+ 0.002*\"investor\" + 0.002*\"benefit\" + 0.002*\"stream\"'),\n",
      " (14,\n",
      "  '0.356*\"platform\" + 0.157*\"offer\" + 0.157*\"data\" + 0.024*\"logistics\" + '\n",
      "  '0.024*\"headquarters\" + 0.001*\"border\" + 0.001*\"tend\" + 0.001*\"stream\" + '\n",
      "  '0.001*\"aligned\" + 0.001*\"feel\"'),\n",
      " (15,\n",
      "  '0.446*\"local\" + 0.153*\"border\" + 0.063*\"offer\" + 0.051*\"community\" + '\n",
      "  '0.042*\"leader\" + 0.039*\"customer\" + 0.001*\"resource\" + 0.001*\"product\" + '\n",
      "  '0.001*\"competitor\" + 0.001*\"benefit\"'),\n",
      " (16,\n",
      "  '0.429*\"user\" + 0.037*\"application\" + 0.037*\"content\" + 0.037*\"interface\" + '\n",
      "  '0.037*\"return\" + 0.001*\"customer\" + 0.001*\"quality\" + 0.001*\"wish\" + '\n",
      "  '0.001*\"tend\" + 0.001*\"loyal\"'),\n",
      " (17,\n",
      "  '0.401*\"quality\" + 0.097*\"price\" + 0.026*\"compared\" + 0.002*\"offer\" + '\n",
      "  '0.002*\"competitor\" + 0.002*\"tend\" + 0.002*\"stream\" + 0.002*\"advocacy\" + '\n",
      "  '0.002*\"initiative\" + 0.002*\"aligned\"'),\n",
      " (18,\n",
      "  '0.567*\"offer\" + 0.091*\"innovation\" + 0.073*\"affiliation\" + '\n",
      "  '0.036*\"technology\" + 0.036*\"achievement\" + 0.023*\"revenue\" + '\n",
      "  '0.023*\"operational\" + 0.019*\"compared\" + 0.019*\"trend\" + '\n",
      "  '0.000*\"capability\"'),\n",
      " (19,\n",
      "  '0.366*\"brand\" + 0.335*\"competitor\" + 0.001*\"customer\" + 0.001*\"service\" + '\n",
      "  '0.001*\"offer\" + 0.001*\"existing\" + 0.001*\"way\" + 0.001*\"stream\" + '\n",
      "  '0.001*\"aligned\" + 0.001*\"loyal\"'),\n",
      " (20,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (21,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (22,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (23,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (24,\n",
      "  '0.819*\"proposition\" + 0.078*\"vision\" + 0.001*\"investor\" + 0.000*\"wish\" + '\n",
      "  '0.000*\"initiative\" + 0.000*\"aligned\" + 0.000*\"tend\" + 0.000*\"stream\" + '\n",
      "  '0.000*\"loyal\" + 0.000*\"advocacy\"'),\n",
      " (25,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (26,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (27,\n",
      "  '0.770*\"resource\" + 0.001*\"owner\" + 0.001*\"treatment\" + 0.001*\"allocate\" + '\n",
      "  '0.001*\"aligned\" + 0.001*\"tend\" + 0.001*\"stream\" + 0.001*\"loyal\" + '\n",
      "  '0.001*\"advocacy\" + 0.001*\"wish\"'),\n",
      " (28,\n",
      "  '0.296*\"portfolio\" + 0.203*\"community\" + 0.040*\"infrastructure\" + '\n",
      "  '0.001*\"customer\" + 0.001*\"channel\" + 0.001*\"product\" + 0.001*\"offer\" + '\n",
      "  '0.001*\"owner\" + 0.001*\"resource\" + 0.001*\"loyal\"'),\n",
      " (29,\n",
      "  '0.242*\"change\" + 0.002*\"proposition\" + 0.002*\"stakeholder\" + 0.002*\"wish\" + '\n",
      "  '0.002*\"customizing\" + 0.002*\"aligned\" + 0.002*\"tend\" + 0.002*\"stream\" + '\n",
      "  '0.002*\"loyal\" + 0.002*\"advocacy\"'),\n",
      " (30,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (31,\n",
      "  '0.494*\"product\" + 0.396*\"service\" + 0.032*\"way\" + 0.000*\"stream\" + '\n",
      "  '0.000*\"advocacy\" + 0.000*\"loyal\" + 0.000*\"treatment\" + 0.000*\"tend\" + '\n",
      "  '0.000*\"allocate\" + 0.000*\"aligned\"'),\n",
      " (32,\n",
      "  '0.757*\"stakeholder\" + 0.110*\"benefit\" + 0.000*\"proposition\" + 0.000*\"wish\" '\n",
      "  '+ 0.000*\"customizing\" + 0.000*\"aligned\" + 0.000*\"tend\" + 0.000*\"stream\" + '\n",
      "  '0.000*\"loyal\" + 0.000*\"advocacy\"'),\n",
      " (33,\n",
      "  '0.389*\"life\" + 0.002*\"proposition\" + 0.002*\"resource\" + 0.002*\"stakeholder\" '\n",
      "  '+ 0.002*\"master\" + 0.002*\"owner\" + 0.002*\"investor\" + 0.002*\"stream\" + '\n",
      "  '0.002*\"tend\" + 0.002*\"associated\"'),\n",
      " (34,\n",
      "  '0.403*\"chain\" + 0.082*\"reward\" + 0.082*\"loyalty\" + 0.035*\"customized\" + '\n",
      "  '0.001*\"customer\" + 0.001*\"proposition\" + 0.001*\"offer\" + 0.001*\"product\" + '\n",
      "  '0.001*\"service\" + 0.001*\"initiative\"'),\n",
      " (35,\n",
      "  '0.169*\"serve\" + 0.003*\"customer\" + 0.003*\"stakeholder\" + 0.003*\"chain\" + '\n",
      "  '0.003*\"offer\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"stream\" + '\n",
      "  '0.003*\"initiative\" + 0.003*\"tend\"'),\n",
      " (36,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (37,\n",
      "  '0.725*\"investor\" + 0.053*\"return\" + 0.041*\"board\" + 0.012*\"advisor\" + '\n",
      "  '0.012*\"governance\" + 0.012*\"priority\" + 0.001*\"stakeholder\" + 0.000*\"user\" '\n",
      "  '+ 0.000*\"customer\" + 0.000*\"opportunity\"'),\n",
      " (38,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (39,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (40,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (41,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (42,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (43,\n",
      "  '0.340*\"future\" + 0.265*\"image\" + 0.054*\"intermediary\" + 0.027*\"inventory\" + '\n",
      "  '0.001*\"customer\" + 0.001*\"offer\" + 0.001*\"existing\" + 0.001*\"benefit\" + '\n",
      "  '0.001*\"stream\" + 0.001*\"associated\"'),\n",
      " (44,\n",
      "  '0.158*\"mechanism\" + 0.067*\"payoff\" + 0.002*\"offer\" + 0.002*\"investor\" + '\n",
      "  '0.002*\"return\" + 0.002*\"owner\" + 0.002*\"opportunity\" + 0.002*\"customer\" + '\n",
      "  '0.002*\"resource\" + 0.002*\"stream\"'),\n",
      " (45,\n",
      "  '0.532*\"capability\" + 0.202*\"skill\" + 0.001*\"border\" + 0.001*\"resource\" + '\n",
      "  '0.001*\"advocacy\" + 0.001*\"initiative\" + 0.001*\"aligned\" + 0.001*\"tend\" + '\n",
      "  '0.001*\"stream\" + 0.001*\"loyal\"'),\n",
      " (46,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (47,\n",
      "  '0.003*\"future\" + 0.003*\"resource\" + 0.003*\"investor\" + 0.003*\"image\" + '\n",
      "  '0.003*\"owner\" + 0.003*\"advocacy\" + 0.003*\"stream\" + 0.003*\"aligned\" + '\n",
      "  '0.003*\"loyal\" + 0.003*\"tend\"'),\n",
      " (48,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"'),\n",
      " (49,\n",
      "  '0.003*\"treatment\" + 0.003*\"customizing\" + 0.003*\"aligned\" + 0.003*\"tend\" + '\n",
      "  '0.003*\"stream\" + 0.003*\"loyal\" + 0.003*\"advocacy\" + 0.003*\"wish\" + '\n",
      "  '0.003*\"associated\" + 0.003*\"preferentially\"')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-068204b2900b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_LDA_Visualization.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36msave_html\u001b[0;34m(data, fileobj, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fileobj should be a filename or a writable file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_data_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                            \u001b[0mvis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return cls(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mskipkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ascii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/utils.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    }
   ],
   "source": [
    "num_topics=50\n",
    "runs = range(1, 6, 1)\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "\n",
    "    # Print the Keyword in the 10 topics\n",
    "    pprint(lda_model.print_topics(num_topics=-1))\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "    # Print out words per topic\n",
    "    top_words_per_topic = []\n",
    "    for t in range(lda_model.num_topics):\n",
    "        top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 10)])\n",
    "\n",
    "    pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"output/\"+str(num_topics)+\"_\"+str(run)+\"_top_words.csv\")\n",
    "\n",
    "    # Visualize the topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    vis\n",
    "\n",
    "    # Calculate Document Topic Weights and Print to CSV\n",
    "\n",
    "    df_doctop = pd.DataFrame(np.zeros((len(data),num_topics),dtype=float), index=np.arange(len(data)), columns=[list(range(num_topics))])\n",
    "    count=0\n",
    "\n",
    "    for a in data:\n",
    "        doc_topic_weights = getDocTopicWeight(lda_model, a)\n",
    "        for b in doc_topic_weights:\n",
    "            # df_doctop = pd.DataFrame(b columns=range(20))\n",
    "            df_doctop.at[count, b[0]] = b[1]\n",
    "\n",
    "            # df_doctop.append(b[1]: doc_topic_weights)\n",
    "        # print(doc_topic_weights[:][1])\n",
    "        count=count+1\n",
    "\n",
    "    df_doctop['Name']=df['name']\n",
    "    df_doctop['Assertion']=data\n",
    "    df_doctop.to_csv(\"output/\"+str(num_topics)+\"_\"+str(run)+\"_document_topic_weights.csv\")\n",
    "    \n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "    pyLDAvis.save_html(vis, \"output/\" + str(num_topics) + '_LDA_Visualization.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
