{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prerequisites â€“ Download nltk stopwords and spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgetanev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en import English\n",
    "\n",
    "parser = English()\n",
    "ss = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "def getDocTopicWeight(lda_model_in, new_doc):\n",
    "    doc = prepare_text_for_lda(new_doc)\n",
    "    new_doc_bow = id2word.doc2bow(doc)\n",
    "    return lda_model_in.get_document_topics(new_doc_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "sw = pd.read_csv('VP_assertions_stop_words_5303_W2021.txt')\n",
    "\n",
    "# Convert to list\n",
    "sw_data = sw.Archive.tolist()\n",
    "# stop_words.extend(sw_data)\n",
    "stop_words = sw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Assrtions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('230 VP assertions corpus - Group 6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G6A002</td>\n",
       "      <td>Access resources required to scale at relative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A003</td>\n",
       "      <td>Adapt offers to each market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A005</td>\n",
       "      <td>Align interests of investors, the company top ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A006</td>\n",
       "      <td>Allow resource owners to make money using your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A007</td>\n",
       "      <td>Apply big data analytics to produce insightful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name                                            content\n",
       "0  G6A002  Access resources required to scale at relative...\n",
       "1    A003                        Adapt offers to each market\n",
       "2    A005  Align interests of investors, the company top ...\n",
       "3    A006  Allow resource owners to make money using your...\n",
       "4    A007  Apply big data analytics to produce insightful..."
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Access resources required to scale at relatively low cost or for free by '\n",
      " 'creating benefits for the resource owners that they cannot create alone']\n"
     ]
    }
   ],
   "source": [
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words and Clean-up text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['access', 'resources', 'required', 'to', 'scale', 'at', 'relatively', 'low', 'cost', 'or', 'for', 'free', 'by', 'creating', 'benefits', 'for', 'the', 'resource', 'owners', 'that', 'they', 'cannot', 'create', 'alone']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize each sentence into a list of words, removing punctuations and unnecessary characters \n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'resources', 'required', 'to', 'scale', 'at', 'relatively', 'low', 'cost', 'or', 'for', 'free', 'by', 'creating', 'benefits', 'for', 'the', 'resource_owners', 'that', 'they', 'cannot', 'create', 'alone']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords, Make Bigrams & Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def stem_words(texts):\n",
    "    return [[ss.stem(word) for word in simple_preprocess(str(doc))] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data)\n",
    "\n",
    "data_stemmed = stem_words(data_words_nostops)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_stemmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['resourc', 'low', 'benefit', 'resourc', 'owner']]\n"
     ]
    }
   ],
   "source": [
    "pprint(data_words_bigrams[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data_words_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 2)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benefit'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('benefit', 1), ('low', 1), ('owner', 1), ('resourc', 2)]]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the topics in LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.304*\"chain\" + 0.193*\"portfolio\" + 0.193*\"life\" + 0.026*\"infrastructur\" + '\n",
      "  '0.001*\"custom\" + 0.001*\"proposit\" + 0.001*\"offer\" + 0.001*\"resourc\" + '\n",
      "  '0.001*\"stakehold\" + 0.001*\"owner\"'),\n",
      " (1,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (2,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (3,\n",
      "  '0.494*\"proposit\" + 0.317*\"employe\" + 0.086*\"investor\" + 0.046*\"vision\" + '\n",
      "  '0.000*\"master\" + 0.000*\"list\" + 0.000*\"group\" + 0.000*\"order\" + '\n",
      "  '0.000*\"alloc\" + 0.000*\"definit\"'),\n",
      " (4,\n",
      "  '0.746*\"resourc\" + 0.001*\"aggress\" + 0.001*\"defin\" + 0.001*\"order\" + '\n",
      "  '0.001*\"list\" + 0.001*\"social\" + 0.001*\"group\" + 0.001*\"emerg\" + '\n",
      "  '0.001*\"effici\" + 0.001*\"preferenti\"'),\n",
      " (5,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (6,\n",
      "  '0.616*\"channel\" + 0.001*\"custom\" + 0.001*\"product\" + 0.001*\"investor\" + '\n",
      "  '0.001*\"owner\" + 0.001*\"list\" + 0.001*\"order\" + 0.001*\"group\" + '\n",
      "  '0.001*\"alloc\" + 0.001*\"definit\"'),\n",
      " (7,\n",
      "  '0.391*\"qualiti\" + 0.095*\"price\" + 0.079*\"compar\" + 0.001*\"offer\" + '\n",
      "  '0.001*\"competitor\" + 0.001*\"alloc\" + 0.001*\"preferenti\" + 0.001*\"order\" + '\n",
      "  '0.001*\"increas\" + 0.001*\"group\"'),\n",
      " (8,\n",
      "  '0.744*\"benefit\" + 0.003*\"proposit\" + 0.001*\"product\" + 0.001*\"custom\" + '\n",
      "  '0.001*\"competitor\" + 0.001*\"stakehold\" + 0.001*\"social\" + 0.001*\"emerg\" + '\n",
      "  '0.001*\"order\" + 0.001*\"list\"'),\n",
      " (9,\n",
      "  '0.506*\"capabl\" + 0.204*\"innov\" + 0.052*\"revenu\" + 0.001*\"stakehold\" + '\n",
      "  '0.001*\"group\" + 0.001*\"emerg\" + 0.001*\"order\" + 0.001*\"list\" + '\n",
      "  '0.001*\"social\" + 0.001*\"preferenti\"'),\n",
      " (10,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (11,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (12,\n",
      "  '0.208*\"investor\" + 0.165*\"custom\" + 0.149*\"user\" + 0.136*\"stakehold\" + '\n",
      "  '0.130*\"data\" + 0.013*\"solut\" + 0.013*\"payment\" + 0.013*\"trend\" + '\n",
      "  '0.013*\"pattern\" + 0.013*\"bound\"'),\n",
      " (13,\n",
      "  '0.377*\"exist\" + 0.002*\"custom\" + 0.002*\"benefit\" + 0.002*\"emerg\" + '\n",
      "  '0.002*\"aggress\" + 0.002*\"alloc\" + 0.002*\"order\" + 0.002*\"list\" + '\n",
      "  '0.002*\"social\" + 0.002*\"group\"'),\n",
      " (14,\n",
      "  '0.166*\"oper\" + 0.070*\"low\" + 0.003*\"capabl\" + 0.003*\"resourc\" + '\n",
      "  '0.003*\"border\" + 0.003*\"benefit\" + 0.003*\"owner\" + 0.003*\"list\" + '\n",
      "  '0.003*\"group\" + 0.003*\"order\"'),\n",
      " (15,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (16,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (17,\n",
      "  '0.003*\"proposit\" + 0.003*\"owner\" + 0.003*\"resourc\" + 0.003*\"offer\" + '\n",
      "  '0.003*\"investor\" + 0.003*\"benefit\" + 0.003*\"alloc\" + 0.003*\"social\" + '\n",
      "  '0.003*\"emerg\" + 0.003*\"list\"'),\n",
      " (18,\n",
      "  '0.613*\"compani\" + 0.099*\"investor\" + 0.024*\"compar\" + 0.001*\"stakehold\" + '\n",
      "  '0.001*\"proposit\" + 0.001*\"offer\" + 0.001*\"list\" + 0.001*\"social\" + '\n",
      "  '0.001*\"order\" + 0.001*\"preferenti\"'),\n",
      " (19,\n",
      "  '0.358*\"platform\" + 0.158*\"data\" + 0.136*\"offer\" + 0.058*\"mechan\" + '\n",
      "  '0.025*\"logist\" + 0.001*\"compar\" + 0.001*\"cybersecur\" + 0.001*\"custom\" + '\n",
      "  '0.001*\"competitor\" + 0.001*\"investor\"'),\n",
      " (20,\n",
      "  '0.376*\"futur\" + 0.294*\"imag\" + 0.001*\"exist\" + 0.001*\"custom\" + '\n",
      "  '0.001*\"offer\" + 0.001*\"benefit\" + 0.001*\"way\" + 0.001*\"social\" + '\n",
      "  '0.001*\"emerg\" + 0.001*\"alloc\"'),\n",
      " (21,\n",
      "  '0.067*\"return\" + 0.067*\"applic\" + 0.067*\"content\" + 0.067*\"interfac\" + '\n",
      "  '0.002*\"resourc\" + 0.002*\"user\" + 0.002*\"qualiti\" + 0.002*\"custom\" + '\n",
      "  '0.002*\"owner\" + 0.002*\"futur\"'),\n",
      " (22,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (23,\n",
      "  '0.441*\"local\" + 0.151*\"border\" + 0.066*\"technolog\" + 0.059*\"trend\" + '\n",
      "  '0.049*\"communiti\" + 0.042*\"leader\" + 0.001*\"custom\" + 0.001*\"offer\" + '\n",
      "  '0.001*\"compani\" + 0.001*\"resourc\"'),\n",
      " (24,\n",
      "  '0.151*\"loyalti\" + 0.151*\"reward\" + 0.002*\"custom\" + 0.002*\"owner\" + '\n",
      "  '0.002*\"resourc\" + 0.002*\"return\" + 0.002*\"group\" + 0.002*\"social\" + '\n",
      "  '0.002*\"aggress\" + 0.002*\"list\"'),\n",
      " (25,\n",
      "  '0.336*\"govern\" + 0.192*\"investor\" + 0.104*\"return\" + 0.081*\"board\" + '\n",
      "  '0.024*\"advisor\" + 0.001*\"alloc\" + 0.001*\"group\" + 0.001*\"preferenti\" + '\n",
      "  '0.001*\"order\" + 0.001*\"social\"'),\n",
      " (26,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (27,\n",
      "  '0.791*\"stakehold\" + 0.053*\"chang\" + 0.001*\"proposit\" + 0.001*\"emerg\" + '\n",
      "  '0.001*\"alloc\" + 0.001*\"order\" + 0.001*\"list\" + 0.001*\"social\" + '\n",
      "  '0.001*\"group\" + 0.001*\"aggress\"'),\n",
      " (28,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (29,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (30,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (31,\n",
      "  '0.908*\"supplier\" + 0.000*\"custom\" + 0.000*\"proposit\" + 0.000*\"effici\" + '\n",
      "  '0.000*\"definit\" + 0.000*\"order\" + 0.000*\"list\" + 0.000*\"social\" + '\n",
      "  '0.000*\"group\" + 0.000*\"emerg\"'),\n",
      " (32,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (33,\n",
      "  '0.003*\"resourc\" + 0.003*\"custom\" + 0.003*\"investor\" + 0.003*\"proposit\" + '\n",
      "  '0.003*\"owner\" + 0.003*\"social\" + 0.003*\"emerg\" + 0.003*\"list\" + '\n",
      "  '0.003*\"defin\" + 0.003*\"order\"'),\n",
      " (34,\n",
      "  '0.364*\"opportun\" + 0.270*\"invest\" + 0.238*\"investor\" + 0.011*\"payoff\" + '\n",
      "  '0.000*\"return\" + 0.000*\"offer\" + 0.000*\"group\" + 0.000*\"order\" + '\n",
      "  '0.000*\"aggress\" + 0.000*\"social\"'),\n",
      " (35,\n",
      "  '0.003*\"resourc\" + 0.003*\"owner\" + 0.003*\"aggress\" + 0.003*\"defin\" + '\n",
      "  '0.003*\"order\" + 0.003*\"list\" + 0.003*\"social\" + 0.003*\"group\" + '\n",
      "  '0.003*\"emerg\" + 0.003*\"effici\"'),\n",
      " (36,\n",
      "  '0.003*\"servic\" + 0.003*\"owner\" + 0.003*\"resourc\" + 0.003*\"product\" + '\n",
      "  '0.003*\"order\" + 0.003*\"group\" + 0.003*\"social\" + 0.003*\"effici\" + '\n",
      "  '0.003*\"list\" + 0.003*\"alloc\"'),\n",
      " (37,\n",
      "  '0.373*\"brand\" + 0.342*\"competitor\" + 0.001*\"custom\" + 0.001*\"servic\" + '\n",
      "  '0.001*\"social\" + 0.001*\"emerg\" + 0.001*\"list\" + 0.001*\"order\" + '\n",
      "  '0.001*\"aggress\" + 0.001*\"alloc\"'),\n",
      " (38,\n",
      "  '0.342*\"resourc\" + 0.250*\"owner\" + 0.178*\"investor\" + 0.070*\"master\" + '\n",
      "  '0.001*\"custom\" + 0.001*\"proposit\" + 0.001*\"benefit\" + 0.001*\"social\" + '\n",
      "  '0.001*\"list\" + 0.001*\"emerg\"'),\n",
      " (39,\n",
      "  '0.907*\"custom\" + 0.038*\"habit\" + 0.000*\"partner\" + 0.000*\"offer\" + '\n",
      "  '0.000*\"list\" + 0.000*\"group\" + 0.000*\"effici\" + 0.000*\"social\" + '\n",
      "  '0.000*\"order\" + 0.000*\"defin\"'),\n",
      " (40,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (41,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (42,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (43,\n",
      "  '0.336*\"affili\" + 0.169*\"achiev\" + 0.002*\"offer\" + 0.002*\"custom\" + '\n",
      "  '0.002*\"servic\" + 0.002*\"product\" + 0.002*\"preferenti\" + 0.002*\"order\" + '\n",
      "  '0.002*\"alloc\" + 0.002*\"social\"'),\n",
      " (44,\n",
      "  '0.003*\"definit\" + 0.003*\"defin\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"emerg\" + 0.003*\"effici\" + 0.003*\"aggress\" + '\n",
      "  '0.003*\"alloc\" + 0.003*\"increas\"'),\n",
      " (45,\n",
      "  '0.840*\"offer\" + 0.013*\"headquart\" + 0.000*\"resourc\" + 0.000*\"benefit\" + '\n",
      "  '0.000*\"border\" + 0.000*\"owner\" + 0.000*\"stakehold\" + 0.000*\"communiti\" + '\n",
      "  '0.000*\"preferenti\" + 0.000*\"order\"'),\n",
      " (46,\n",
      "  '0.372*\"skill\" + 0.165*\"cybersecur\" + 0.002*\"capabl\" + 0.002*\"resourc\" + '\n",
      "  '0.002*\"channel\" + 0.002*\"offer\" + 0.002*\"order\" + 0.002*\"social\" + '\n",
      "  '0.002*\"alloc\" + 0.002*\"effici\"'),\n",
      " (47,\n",
      "  '0.142*\"intermediari\" + 0.072*\"inventori\" + 0.003*\"custom\" + 0.003*\"effici\" '\n",
      "  '+ 0.003*\"alloc\" + 0.003*\"order\" + 0.003*\"list\" + 0.003*\"social\" + '\n",
      "  '0.003*\"group\" + 0.003*\"treatment\"'),\n",
      " (48,\n",
      "  '0.497*\"product\" + 0.398*\"servic\" + 0.032*\"way\" + 0.000*\"effici\" + '\n",
      "  '0.000*\"defin\" + 0.000*\"order\" + 0.000*\"list\" + 0.000*\"social\" + '\n",
      "  '0.000*\"group\" + 0.000*\"emerg\"'),\n",
      " (49,\n",
      "  '0.543*\"partner\" + 0.165*\"standard\" + 0.120*\"communiti\" + '\n",
      "  '0.014*\"cyberattack\" + 0.002*\"benefit\" + 0.001*\"invest\" + 0.001*\"custom\" + '\n",
      "  '0.001*\"compani\" + 0.001*\"list\" + 0.001*\"alloc\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics(num_topics=-1))\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Top Topics Words to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(lda_model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 10)])\n",
    "\n",
    "pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(str(num_topics)+\"_top_words.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data, kwds)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text/html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     formatter.for_type(PreparedData,\n\u001b[0;32m--> 311\u001b[0;31m                        lambda data, kwds=kwargs: prepared_data_to_html(data, **kwds))\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                            \u001b[0mvis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return cls(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mskipkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ascii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/PycharmProjects/Timg-5303-ML/venv/lib/python3.9/site-packages/pyLDAvis/utils.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                        x                   y  topics  cluster       Freq\n",
       "topic                                                                    \n",
       "39    -0.328834+0.000000j -0.164144+0.000000j       1        1  13.059709\n",
       "48    -0.207379+0.000000j -0.188400+0.000000j       2        1   9.047496\n",
       "3     -0.316624+0.000000j  0.114991+0.000000j       3        1   8.680734\n",
       "31    -0.197415+0.000000j -0.172730+0.000000j       4        1   5.120734\n",
       "45    -0.109940+0.000000j -0.157368+0.000000j       5        1   4.858637\n",
       "12    -0.261641+0.000000j  0.066934+0.000000j       6        1   4.780161\n",
       "38    -0.188115+0.000000j  0.236445+0.000000j       7        1   4.728322\n",
       "23    -0.032614+0.000000j -0.074493+0.000000j       8        1   4.570810\n",
       "27    -0.139070+0.000000j -0.060528+0.000000j       9        1   4.301633\n",
       "34    -0.243149+0.000000j  0.201665+0.000000j      10        1   3.704871\n",
       "49    -0.075020+0.000000j -0.102587+0.000000j      11        1   3.667506\n",
       "18    -0.075762+0.000000j  0.127209+0.000000j      12        1   3.084290\n",
       "9     -0.008242+0.000000j -0.049831+0.000000j      13        1   2.641958\n",
       "19    -0.018967+0.000000j -0.087453+0.000000j      14        1   2.435856\n",
       "37     0.015627+0.000000j -0.036215+0.000000j      15        1   2.313318\n",
       "4     -0.053498+0.000000j  0.102748+0.000000j      16        1   2.032182\n",
       "8     -0.012933+0.000000j -0.049297+0.000000j      17        1   1.890550\n",
       "0      0.024122+0.000000j -0.033070+0.000000j      18        1   1.804822\n",
       "25    -0.088950+0.000000j  0.171705+0.000000j      19        1   1.687581\n",
       "20     0.035998+0.000000j -0.025606+0.000000j      20        1   1.438887\n",
       "6      0.050324+0.000000j -0.017552+0.000000j      21        1   1.419986\n",
       "7      0.069311+0.000000j -0.003213+0.000000j      22        1   1.074347\n",
       "46     0.073044+0.000000j -0.006494+0.000000j      23        1   0.951595\n",
       "13     0.087206+0.000000j  0.002954+0.000000j      24        1   0.892630\n",
       "43     0.078086+0.000000j -0.003719+0.000000j      25        1   0.832103\n",
       "24     0.089567+0.000000j  0.005711+0.000000j      26        1   0.757702\n",
       "14     0.088286+0.000000j  0.006965+0.000000j      27        1   0.693869\n",
       "21     0.087354+0.000000j  0.016274+0.000000j      28        1   0.635414\n",
       "47     0.087526+0.000000j  0.007265+0.000000j      29        1   0.523382\n",
       "33     0.074840+0.000000j  0.008185+0.000000j      30        1   0.306849\n",
       "35     0.074843+0.000000j  0.008183+0.000000j      31        1   0.305491\n",
       "17     0.074842+0.000000j  0.008183+0.000000j      32        1   0.305126\n",
       "36     0.074843+0.000000j  0.008182+0.000000j      33        1   0.305106\n",
       "32     0.074843+0.000000j  0.008182+0.000000j      34        1   0.302726\n",
       "42     0.074843+0.000000j  0.008182+0.000000j      35        1   0.302726\n",
       "2      0.074843+0.000000j  0.008182+0.000000j      36        1   0.302726\n",
       "22     0.074843+0.000000j  0.008182+0.000000j      37        1   0.302726\n",
       "15     0.074843+0.000000j  0.008182+0.000000j      38        1   0.302726\n",
       "5      0.074843+0.000000j  0.008182+0.000000j      39        1   0.302726\n",
       "44     0.074843+0.000000j  0.008182+0.000000j      40        1   0.302726\n",
       "1      0.074843+0.000000j  0.008182+0.000000j      41        1   0.302726\n",
       "41     0.074843+0.000000j  0.008182+0.000000j      42        1   0.302726\n",
       "30     0.074843+0.000000j  0.008182+0.000000j      43        1   0.302726\n",
       "40     0.074843+0.000000j  0.008182+0.000000j      44        1   0.302726\n",
       "26     0.074843+0.000000j  0.008182+0.000000j      45        1   0.302726\n",
       "28     0.074843+0.000000j  0.008182+0.000000j      46        1   0.302726\n",
       "10     0.074843+0.000000j  0.008182+0.000000j      47        1   0.302726\n",
       "29     0.074843+0.000000j  0.008182+0.000000j      48        1   0.302726\n",
       "16     0.074843+0.000000j  0.008182+0.000000j      49        1   0.302726\n",
       "11     0.074843+0.000000j  0.008182+0.000000j      50        1   0.302726, topic_info=         Term        Freq       Total Category  logprob  loglift\n",
       "8      custom  134.000000  134.000000  Default  30.0000  30.0000\n",
       "13   supplier   50.000000   50.000000  Default  29.0000  29.0000\n",
       "4       offer   47.000000   47.000000  Default  28.0000  28.0000\n",
       "24  stakehold   43.000000   43.000000  Default  27.0000  27.0000\n",
       "3     resourc   33.000000   33.000000  Default  26.0000  26.0000\n",
       "..        ...         ...         ...      ...      ...      ...\n",
       "24  stakehold    0.010804   43.747699  Topic50  -5.6937  -2.5061\n",
       "25     capabl    0.010804   14.978427  Topic50  -5.6937  -1.4343\n",
       "26      skill    0.010804    4.565059  Topic50  -5.6937  -0.2461\n",
       "27       oper    0.010804    2.023158  Topic50  -5.6937   0.5676\n",
       "29   opportun    0.010804   15.119875  Topic50  -5.6937  -1.4437\n",
       "\n",
       "[2862 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "77       25  0.434872   achiev\n",
       "78       25  0.794210   affili\n",
       "0        17  0.950487  benefit\n",
       "68       19  0.443523    board\n",
       "23        8  0.860944   border\n",
       "...     ...       ...      ...\n",
       "14        6  0.233947    trend\n",
       "14        8  0.701840    trend\n",
       "15        6  0.956512     user\n",
       "81        3  0.790422   vision\n",
       "43        2  0.779905      way\n",
       "\n",
       "[80 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[40, 49, 4, 32, 46, 13, 39, 24, 28, 35, 50, 19, 10, 20, 38, 5, 9, 1, 26, 21, 7, 8, 47, 14, 44, 25, 15, 22, 48, 34, 36, 18, 37, 33, 43, 3, 23, 16, 6, 45, 2, 42, 31, 41, 27, 29, 11, 30, 17, 12])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Document Topic Weights and Print to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doctop = pd.DataFrame(np.zeros((len(data),num_topics),dtype=float), index=np.arange(len(data)), columns=[list(range(num_topics))])\n",
    "count=0\n",
    "\n",
    "for a in data:\n",
    "    doc_topic_weights = getDocTopicWeight(lda_model, a)\n",
    "    for b in doc_topic_weights:\n",
    "        # df_doctop = pd.DataFrame(b columns=range(20))\n",
    "        df_doctop.at[count, b[0]] = b[1]\n",
    "\n",
    "        # df_doctop.append(b[1]: doc_topic_weights)\n",
    "    # print(doc_topic_weights[:][1])\n",
    "    count=count+1\n",
    "\n",
    "df_doctop['Name']=df['name']\n",
    "df_doctop['Assertion']=data\n",
    "df_doctop.to_csv(str(num_topics)+\"_document_topic_weights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
